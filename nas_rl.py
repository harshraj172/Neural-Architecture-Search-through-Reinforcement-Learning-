# -*- coding: utf-8 -*-
"""NAS-RL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IsehyRWKpn_DCEG0VvViL_UMaZDOJIWJ
"""

!pip install keras-rl

import numpy as np
import gym
from gym import Env
from gym.spaces import Discrete, Box

from keras.models import Sequential
from keras.layers import Dense, Activation, Flatten
from keras.optimizers import Adam

from rl.agents.dqn import DQNAgent
from rl.policy import EpsGreedyQPolicy
from rl.memory import SequentialMemory

"""#Defining a controller
Here we use LSTM layers to produce the actions which acts as the controller and manages to take appropriate actions based on previous actions to converge faster and avoid local optimas.
"""

# initiate a random sequence with length atleast 4
from numpy import array
#import random
lst_seq = array([[10.0, 128.0, 1.0, 1.0], [9.0, 130.0, 1.3, 1.0], [11.0, 127.0, 1.0, 1.2], [12.0, 126.0, 1.15, 1.32]])
def get_action(action):
  
  #inserting noise to have a global search perspective
  if action == 0:
    return array([random.randint(1, 7), random.randint(90, 280), random.randint(1, 8), random.randint(1, 10)])

  # lstm example
  from keras.models import Sequential
  from keras.layers import LSTM
  from keras.layers import Dense
    
  # split a univariate sequence into samples
  def split_sequence(sequence, n_steps):
     x, y = list(), list()
     for i in range(len(sequence)):
      # find the end of this pattern
      end_ix = i + n_steps
      # check if we are beyond the sequence
      if end_ix > len(sequence)-1:
        break
      # gather input and output parts of the pattern
      seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]
      x.append(seq_x)
      y.append(seq_y)
     return array(x), array(y)
    
  # choose a number of time steps
  n_steps = 3
  # split into samples
  x, y = split_sequence(lst_seq, n_steps)
  # reshape from [samples, timesteps] into [samples, timesteps, features]
  n_features = 4
  x = x.reshape((x.shape[0], x.shape[1], n_features))

  # define model
  controller = Sequential()
  controller.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))
  controller.add(Dense(4))
  controller.compile(optimizer='adam', loss='mse')
  # fit model
  controller.fit(x, y, epochs=200, verbose=0)

  # demonstrate prediction
  x_input = array([lst_seq[0], lst_seq[1], lst_seq[2]])
  x_input = x_input.reshape((1, n_steps, n_features))
  ypred = model.predict(x_input, verbose=0)

  #reconstructing
  lst_seq[0] = lst_seq[1]
  lst_seq[1] = lst_seq[2]
  lst_seq[2] = lst_seq[3]
  lst_seq[3] = y_pred

  #converting float to int
  y_pred[0] = round(y_pred[0])
  y_pred[1] = round(y_pred[1])
  y_pred[2] = round(y_pred[2])
  y_pred[3] = round(y_pred[3])

  return ypred

"""#Reward Function
In every iteration we use the parameters produced by the get_action() function to predict the accuracy on the test set.
"""

# preparing the test and train data
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X = scaler.fit_transform(X)
y = scaler.fit_transform(y)
X_train, X_test, y_train, y_test = traintestsplit(X, y, test_size=0.3)

from keras.utils import to_categorical

y_train = to_categorical(y_train)
y_test  = to_categorical(y_test)

def get_reward(cnn_config):

  # setting up the child network with the actions values
  child_model = Sequential()
  for i in max_layer:
    for cnn_num_filters, filter_size, max_pool_ksize, cnn_dropout_rates in cnn_config[i]:
      child_model.add.tf.layers.conv1d(
                          pool_out,
                          filters=cnn_num_filters,
                          kernel_size=filter_size,
                          strides=1,
                          padding="SAME",
                          name="conv_out_"+str(i),
                          activation=tf.nn.relu,
                          kernel_initializer=tf.contrib.layers.xavier_initializer(),
                          bias_initializer=tf.zeros_initializer
                      )
      child_model.add.tf.layers.max_pooling1d(
                          conv_out,
                          pool_size=max_pool_ksize,
                          strides=1,
                          padding='SAME',
                          name="max_pool_"+str(idd)
                   )
      child_model.add.pool_out = tf.nn.dropout(pool_out, cnn_dropout_rates)

  child_model.add(Flatten())
  child_model.add(Dense(num_classes, activation='softmax'))

  #training and getting the reward from the upgraded child network
  child_model.compile(optimizer='rmsprop',
                  loss='categorical_crossentropy',
                  metrics = ['accuracy'])

  child_model.fit(X_train, y_train, epochs=5, batch_size=128)

  test_loss, test_acc = child_model.evaluate(X_test, y_test)
  reward = test_acc
  return reward

"""#Clone the environment class from OpenAI gym
We used the general way described in the openAI gym to describe the environment.
"""

class ENV(Env):
    def __init__(self):
        # Actions we can take
        self.action_space = Discrete(2)
        # observation_space
        ob1 = Box(low=np.array([5]), high=np.array([15]))
        ob2 = Box(low=np.array([100]), high=np.array([200]))
        ob3 = Box(low=np.array([5]), high=np.array([5]))
        ob4 = Box(low=np.array([5]), high=np.array([5]))
        self.observation_space = array([ob1, ob2, ob3, ob4])
        # Set start parameters
        self.state = lst_seq[3]
        # Set episode length
        self.episode_length = 50
        
    def step(self, action):
        # Apply action
        self.state = get_action(action) 
        # Reduce episode length by 1 second
        self.episode_length -= 1
        
        # Calculate reward
        reward += get_reward(self.state)
        
        # Check if shower is done
        if self.episode_length <= 0: 
            done = True
        else:
            done = False
        
        # Set placeholder for info
        info = {}
        
        # Return step information
        return self.state, reward, done, info

    def render(self):
        # Implement viz
        pass
    
    def reset(self):
        # Reset shower temperature
        self.state = lst_seq[3]
        # Reset shower time
        self.episode_length = 50 
        return self.state

env = ENV()
nb_actions = env.action_space.n

"""#Creating a Deep Learning environment"""

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adam

states = env.observation_space.shape
actions = env.action_space.n

def build_model(states, actions):
    model = Sequential()    
    model.add(Dense(24, activation='relu', input_shape=states))
    model.add(Dense(24, activation='relu'))
    model.add(Dense(actions, activation='linear'))
    return model

del model

model = build_model(states, actions)
model.summary()

"""#Building Agent
We get all the permutations of states which can be possible after a particular state.
"""

from rl.agents import DQNAgent
from rl.policy import BoltzmannQPolicy
from rl.memory import SequentialMemory

def build_agent(model, actions):
    policy = BoltzmannQPolicy()
    memory = SequentialMemory(limit=50000, window_length=1)
    dqn = DQNAgent(model=model, memory=memory, policy=policy, 
                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)
    return dqn

dqn = build_agent(model, actions) 
dqn.compile(Adam(lr=1e-3), metrics=['mae'])
dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)

"""#Testing the model with the best found parameters

"""

scores = dqn.test(env, nb_episodes=100, visualize=False)
print(np.mean(scores.history['episode_reward']))